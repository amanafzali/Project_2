{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hvplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports to get show started\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from numpy.random import seed\n",
    "from pathlib import Path\n",
    "\n",
    "# Import required preprocessing and Keras modules\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO after model experimentation\n",
    "# Define random seed for reproducibility\n",
    "\n",
    "# seed(1)\n",
    "# random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 35513 entries, 2017-01-01 00:00:00 to 2021-01-15 00:00:00\n",
      "Data columns (total 24 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Close                 35513 non-null  float64\n",
      " 1   Volume                35513 non-null  float64\n",
      " 2   US_Holiday            35513 non-null  float64\n",
      " 3   US_Market_Open        35513 non-null  float64\n",
      " 4   Trail24hr_CloseRatio  35513 non-null  float64\n",
      " 5   Trail12Wk_CloseRatio  35513 non-null  float64\n",
      " 6   Trail52Wk_CloseRatio  35513 non-null  float64\n",
      " 7   Hr_Return             35513 non-null  float64\n",
      " 8   Trail24hr_Return      35513 non-null  float64\n",
      " 9   Trail24hr_Std         35513 non-null  float64\n",
      " 10  Trail12Wk_Return      35513 non-null  float64\n",
      " 11  Trail12Wk_Std         35513 non-null  float64\n",
      " 12  Trail52Wk_Return      35513 non-null  float64\n",
      " 13  Trail52Wk_Std         35513 non-null  float64\n",
      " 14  Trail24hr_VolRatio    35513 non-null  float64\n",
      " 15  Trail12Wk_VolRatio    35513 non-null  float64\n",
      " 16  Trail52Wk_VolRatio    35513 non-null  float64\n",
      " 17  Vol_PctDelta          35513 non-null  float64\n",
      " 18  Bitcoin Trend         35513 non-null  float64\n",
      " 19  Cryptocurrency        35513 non-null  float64\n",
      " 20  crypto all time high  35513 non-null  float64\n",
      " 21  crypto drop           35513 non-null  float64\n",
      " 22  btc halving           35513 non-null  float64\n",
      " 23  Significant_Drawdown  35513 non-null  int64  \n",
      "dtypes: float64(23), int64(1)\n",
      "memory usage: 6.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Read in prepared model dataset created in data_prep notebook \n",
    "\n",
    "model_df = pd.read_csv(Path('./ModelData/model_dataset.csv'),index_col=\"Date_Time\",infer_datetime_format=True,parse_dates=True)\n",
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 35513 entries, 2017-01-01 00:00:00 to 2021-01-15 00:00:00\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   US_Market_Open        35513 non-null  float64\n",
      " 1   Trail24hr_CloseRatio  35513 non-null  float64\n",
      " 2   Trail12Wk_CloseRatio  35513 non-null  float64\n",
      " 3   Trail52Wk_CloseRatio  35513 non-null  float64\n",
      " 4   Hr_Return             35513 non-null  float64\n",
      " 5   Trail24hr_Return      35513 non-null  float64\n",
      " 6   Trail24hr_Std         35513 non-null  float64\n",
      " 7   Trail12Wk_Return      35513 non-null  float64\n",
      " 8   Trail12Wk_Std         35513 non-null  float64\n",
      " 9   Trail52Wk_Return      35513 non-null  float64\n",
      " 10  Trail52Wk_Std         35513 non-null  float64\n",
      " 11  Trail24hr_VolRatio    35513 non-null  float64\n",
      " 12  Trail12Wk_VolRatio    35513 non-null  float64\n",
      " 13  Trail52Wk_VolRatio    35513 non-null  float64\n",
      " 14  Vol_PctDelta          35513 non-null  float64\n",
      " 15  Bitcoin Trend         35513 non-null  float64\n",
      " 16  Cryptocurrency        35513 non-null  float64\n",
      " 17  crypto all time high  35513 non-null  float64\n",
      " 18  crypto drop           35513 non-null  float64\n",
      " 19  btc halving           35513 non-null  float64\n",
      " 20  Significant_Drawdown  35513 non-null  int64  \n",
      "dtypes: float64(20), int64(1)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Last minute pruning of unwanted columns\n",
    "#  remove Close price, US holiday\n",
    "column_2drop_list = ['Close',\n",
    "                     'Volume',\n",
    "                     'US_Holiday']\n",
    "\n",
    "# old: remove 12week numbers to reduce model fittin time for initial model evals\n",
    "#column_2drop_list = ['Close',\n",
    "#                     'US_Holiday',\n",
    "#                     'Trail12Wk_CloseRatio',\n",
    "#                     'Trail12Wk_Return',\n",
    "#                     'Trail12Wk_Std',\n",
    "#                     'Trail12Wk_VolRatio',\n",
    "#                     'Vol_PctDelta']\n",
    "\n",
    "model_df = model_df.drop(columns=column_2drop_list)\n",
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_window_chopper(df, window_len, feature_col_numlist, target_col_num):\n",
    "    \"\"\"\n",
    "    Function chops up dataframe features (X) defined by column numbers\n",
    "    in feature_col_numlist and target (y) values defined by t_col_num\n",
    "    with a rolling window of length window_len.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df) - window_len):\n",
    "        feature_list = []\n",
    "        for feature_col_num in feature_col_numlist:\n",
    "            feature_list.append(df.iloc[i:(i + window_len), feature_col_num])\n",
    "        X.append(feature_list)\n",
    "        y.append(df.iloc[(i + window_len), target_col_num])\n",
    "    return np.array(X).reshape(-1,(len(feature_col_numlist)*window_len)), np.array(y).reshape(-1, 1)\n",
    "#    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X sample values:\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   9.97383408e-01  9.95895542e-01  9.95218306e-01  9.93073726e-01\n",
      "   9.93073726e-01  9.91801344e-01  9.93073726e-01  9.93073726e-01\n",
      "   9.93073726e-01  9.94828382e-01  9.99989739e-01  1.00000000e+00\n",
      "   9.89343186e-01  9.87867314e-01  9.87195538e-01  9.85068246e-01\n",
      "   9.85068246e-01  9.83806121e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.85068246e-01  9.86808758e-01  9.91928507e-01  9.94585076e-01\n",
      "   9.89343186e-01  9.87867314e-01  9.87195538e-01  9.85068246e-01\n",
      "   9.85068246e-01  9.83806121e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.85068246e-01  9.86808758e-01  9.91928507e-01  9.94585076e-01\n",
      "  -2.61659227e-03 -1.49176955e-03 -6.80026789e-04 -2.15488354e-03\n",
      "   0.00000000e+00 -1.28125646e-03  1.28290018e-03  0.00000000e+00\n",
      "   0.00000000e+00  1.76689399e-03  5.18818785e-03  2.67818663e-03\n",
      "   1.68718876e-02  1.56730020e-02  1.94189114e-02  1.04327089e-02\n",
      "   1.87625628e-02  1.28814032e-02  4.69228456e-03  8.25248094e-03\n",
      "   8.89831428e-03  1.50951526e-02  2.04194476e-02  1.92442311e-02\n",
      "   4.92239243e-03  4.93899354e-03  4.82948602e-03  4.68858146e-03\n",
      "   4.30411750e-03  4.24428844e-03  3.80071337e-03  3.71629470e-03\n",
      "   3.71115147e-03  3.57572247e-03  3.68954579e-03  3.65560558e-03\n",
      "   4.67177792e-01  4.65216760e-01  4.64520559e-01  4.62867058e-01\n",
      "   4.65229594e-01  4.63948338e-01  4.65231238e-01  4.64501335e-01\n",
      "   4.64776875e-01  4.66543769e-01  4.71731957e-01  4.74410143e-01\n",
      "   3.76268101e-03  3.76287306e-03  3.76292471e-03  3.76326448e-03\n",
      "   3.76282471e-03  3.76297190e-03  3.76304144e-03  3.76302851e-03\n",
      "   3.76301513e-03  3.76316716e-03  3.76478249e-03  3.76517238e-03\n",
      "   9.51089704e-01  9.49597934e-01  9.64694523e-01  9.71238721e-01\n",
      "   9.70536688e-01  9.69255431e-01  9.68410323e-01  9.68060298e-01\n",
      "   9.66357435e-01  9.68566787e-01  9.73754975e-01  9.76433161e-01\n",
      "   5.90840562e-03  5.90843033e-03  5.90599089e-03  5.90528836e-03\n",
      "   5.90528510e-03  5.90530377e-03  5.90527765e-03  5.90527721e-03\n",
      "   5.90525276e-03  5.90527638e-03  5.90552611e-03  5.90558983e-03\n",
      "   1.04073123e-01  2.10200568e-02  7.78877600e-01  2.27080488e-02\n",
      "   4.80815503e-02  3.75583002e-03  5.66923242e-03  1.23828378e-02\n",
      "   5.91122250e-03  2.03363117e-03  8.66113364e-02  8.42161590e-02\n",
      "   3.88273766e-03  7.84211749e-04  2.90581977e-02  8.47186993e-04\n",
      "   1.79381612e-03  1.40121697e-04  2.11506502e-04  4.61976244e-04\n",
      "   2.20534616e-04  7.58702735e-05  3.23127708e-03  3.14191832e-03\n",
      "   3.43594238e-03  6.93970754e-04  2.57144061e-02  7.49699296e-04\n",
      "   1.58739770e-03  1.23997581e-04  1.87167977e-04  4.08815607e-04\n",
      "   1.95157206e-04  6.71397119e-05  2.85944682e-03  2.78037077e-03\n",
      "   6.78340815e-01 -7.98026079e-01  3.60540198e+01 -9.70845164e-01\n",
      "   1.11737920e+00 -9.21886254e-01  5.09448615e-01  1.18421770e+00\n",
      "  -5.22627801e-01 -6.55971135e-01  4.15895009e+01 -2.76543179e-02\n",
      "   3.10000000e+01  3.00000000e+01  3.20000000e+01  3.40000000e+01\n",
      "   2.90000000e+01  2.90000000e+01  2.90000000e+01  2.90000000e+01\n",
      "   2.90000000e+01  2.70000000e+01  2.70000000e+01  2.80000000e+01\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  5.20000000e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  6.40000000e+01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  5.20000000e+01  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   9.95895542e-01  9.95218306e-01  9.93073726e-01  9.93073726e-01\n",
      "   9.91801344e-01  9.93073726e-01  9.93073726e-01  9.93073726e-01\n",
      "   9.94828382e-01  9.99989739e-01  1.00000000e+00  9.98178376e-01\n",
      "   9.87867314e-01  9.87195538e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.83806121e-01  9.85068246e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.86808758e-01  9.91928507e-01  9.94585076e-01  9.92773316e-01\n",
      "   9.87867314e-01  9.87195538e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.83806121e-01  9.85068246e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.86808758e-01  9.91928507e-01  9.94585076e-01  9.92773316e-01\n",
      "  -1.49176955e-03 -6.80026789e-04 -2.15488354e-03  0.00000000e+00\n",
      "  -1.28125646e-03  1.28290018e-03  0.00000000e+00  0.00000000e+00\n",
      "   1.76689399e-03  5.18818785e-03  2.67818663e-03 -1.82162411e-03\n",
      "   1.56730020e-02  1.94189114e-02  1.04327089e-02  1.87625628e-02\n",
      "   1.28814032e-02  4.69228456e-03  8.25248094e-03  8.89831428e-03\n",
      "   1.50951526e-02  2.04194476e-02  1.92442311e-02  1.37717537e-02\n",
      "   4.93899354e-03  4.82948602e-03  4.68858146e-03  4.30411750e-03\n",
      "   4.24428844e-03  3.80071337e-03  3.71629470e-03  3.71115147e-03\n",
      "   3.57572247e-03  3.68954579e-03  3.65560558e-03  3.64081504e-03\n",
      "   4.65216760e-01  4.64520559e-01  4.62867058e-01  4.65229594e-01\n",
      "   4.63948338e-01  4.65231238e-01  4.64501335e-01  4.64776875e-01\n",
      "   4.66543769e-01  4.71731957e-01  4.74410143e-01  4.72588519e-01\n",
      "   3.76287306e-03  3.76292471e-03  3.76326448e-03  3.76282471e-03\n",
      "   3.76297190e-03  3.76304144e-03  3.76302851e-03  3.76301513e-03\n",
      "   3.76316716e-03  3.76478249e-03  3.76517238e-03  3.76544746e-03\n",
      "   9.49597934e-01  9.64694523e-01  9.71238721e-01  9.70536688e-01\n",
      "   9.69255431e-01  9.68410323e-01  9.68060298e-01  9.66357435e-01\n",
      "   9.68566787e-01  9.73754975e-01  9.76433161e-01  9.74611537e-01\n",
      "   5.90843033e-03  5.90599089e-03  5.90528836e-03  5.90528510e-03\n",
      "   5.90530377e-03  5.90527765e-03  5.90527721e-03  5.90525276e-03\n",
      "   5.90527638e-03  5.90552611e-03  5.90558983e-03  5.90562594e-03\n",
      "   2.10200568e-02  7.78877600e-01  2.27080488e-02  4.80815503e-02\n",
      "   3.75583002e-03  5.66923242e-03  1.23828378e-02  5.91122250e-03\n",
      "   2.03363117e-03  8.66113364e-02  8.42161590e-02  3.47807481e-02\n",
      "   7.84211749e-04  2.90581977e-02  8.47186993e-04  1.79381612e-03\n",
      "   1.40121697e-04  2.11506502e-04  4.61976244e-04  2.20534616e-04\n",
      "   7.58702735e-05  3.23127708e-03  3.14191832e-03  1.29759266e-03\n",
      "   6.93970754e-04  2.57144061e-02  7.49699296e-04  1.58739770e-03\n",
      "   1.23997581e-04  1.87167977e-04  4.08815607e-04  1.95157206e-04\n",
      "   6.71397119e-05  2.85944682e-03  2.78037077e-03  1.14827577e-03\n",
      "  -7.98026079e-01  3.60540198e+01 -9.70845164e-01  1.11737920e+00\n",
      "  -9.21886254e-01  5.09448615e-01  1.18421770e+00 -5.22627801e-01\n",
      "  -6.55971135e-01  4.15895009e+01 -2.76543179e-02 -5.87006241e-01\n",
      "   3.00000000e+01  3.20000000e+01  3.40000000e+01  2.90000000e+01\n",
      "   2.90000000e+01  2.90000000e+01  2.90000000e+01  2.90000000e+01\n",
      "   2.70000000e+01  2.70000000e+01  2.80000000e+01  2.90000000e+01\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  5.20000000e+01  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  6.40000000e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  5.20000000e+01  0.00000000e+00  4.60000000e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Create features (X) and target (y) data window sets\n",
    "\n",
    "window_size = 12 # model dataset is hourly, try half day for starters\n",
    "feature_col_numlist = list(range(model_df.shape[1]-1))\n",
    "target_col_num = (model_df.shape[1] - 1) # 0s based column index\n",
    "X, y = data_window_chopper(model_df, window_size, feature_col_numlist, target_col_num)\n",
    "\n",
    "print(f\"X sample values:\\n {X[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 70% of the data for training, 30% for testing\n",
    "split = int(0.7 * len(X))\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the MinMaxScaler to scale data between 0 and 1.\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# target is already boolean, doesnt need scaling\n",
    "#scaler.fit(y)\n",
    "#y_train = scaler.transform(y_train)\n",
    "#y_test = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train sample values:\n",
      "[[[0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [9.89981801e-01]\n",
      "  [9.84285178e-01]\n",
      "  [9.81692232e-01]\n",
      "  [9.73481237e-01]\n",
      "  [9.73481237e-01]\n",
      "  [9.68609642e-01]\n",
      "  [9.73481237e-01]\n",
      "  [9.73481237e-01]\n",
      "  [9.73481237e-01]\n",
      "  [9.80199324e-01]\n",
      "  [9.99960713e-01]\n",
      "  [1.00000000e+00]\n",
      "  [9.84769243e-01]\n",
      "  [9.82659922e-01]\n",
      "  [9.81699817e-01]\n",
      "  [9.78659484e-01]\n",
      "  [9.78659484e-01]\n",
      "  [9.76855651e-01]\n",
      "  [9.78659484e-01]\n",
      "  [9.78659484e-01]\n",
      "  [9.78659484e-01]\n",
      "  [9.81147029e-01]\n",
      "  [9.88464193e-01]\n",
      "  [9.92260972e-01]\n",
      "  [9.87342852e-01]\n",
      "  [9.85589952e-01]\n",
      "  [9.84792080e-01]\n",
      "  [9.82265486e-01]\n",
      "  [9.82265486e-01]\n",
      "  [9.80766454e-01]\n",
      "  [9.82265486e-01]\n",
      "  [9.82265486e-01]\n",
      "  [9.82265486e-01]\n",
      "  [9.84332699e-01]\n",
      "  [9.90413450e-01]\n",
      "  [9.93568670e-01]\n",
      "  [4.80061731e-01]\n",
      "  [4.84087267e-01]\n",
      "  [4.86992347e-01]\n",
      "  [4.81714103e-01]\n",
      "  [4.89426040e-01]\n",
      "  [4.84840656e-01]\n",
      "  [4.94017306e-01]\n",
      "  [4.89426040e-01]\n",
      "  [4.89426040e-01]\n",
      "  [4.95749432e-01]\n",
      "  [5.07993621e-01]\n",
      "  [4.99010783e-01]\n",
      "  [5.16327726e-01]\n",
      "  [5.14385295e-01]\n",
      "  [5.20454406e-01]\n",
      "  [5.05894990e-01]\n",
      "  [5.19390992e-01]\n",
      "  [5.09862356e-01]\n",
      "  [4.96594373e-01]\n",
      "  [5.02362591e-01]\n",
      "  [5.03408968e-01]\n",
      "  [5.13449066e-01]\n",
      "  [5.22075471e-01]\n",
      "  [5.20171389e-01]\n",
      "  [9.77459268e-02]\n",
      "  [9.80755817e-02]\n",
      "  [9.59010468e-02]\n",
      "  [9.31030482e-02]\n",
      "  [8.54685927e-02]\n",
      "  [8.42805430e-02]\n",
      "  [7.54722944e-02]\n",
      "  [7.37959589e-02]\n",
      "  [7.36938278e-02]\n",
      "  [7.10045596e-02]\n",
      "  [7.32647949e-02]\n",
      "  [7.25908306e-02]\n",
      "  [4.63351057e-01]\n",
      "  [4.62577296e-01]\n",
      "  [4.62302598e-01]\n",
      "  [4.61650179e-01]\n",
      "  [4.62582360e-01]\n",
      "  [4.62076818e-01]\n",
      "  [4.62583009e-01]\n",
      "  [4.62295012e-01]\n",
      "  [4.62403732e-01]\n",
      "  [4.63100892e-01]\n",
      "  [4.65147985e-01]\n",
      "  [4.66204712e-01]\n",
      "  [0.00000000e+00]\n",
      "  [3.38633745e-06]\n",
      "  [7.00384301e-06]\n",
      "  [3.08011667e-05]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [1.84313764e-06]\n",
      "  [9.37247161e-07]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [4.54684564e-01]\n",
      "  [4.54386716e-01]\n",
      "  [4.57400917e-01]\n",
      "  [4.58707539e-01]\n",
      "  [4.58567370e-01]\n",
      "  [4.58311553e-01]\n",
      "  [4.58142818e-01]\n",
      "  [4.58072931e-01]\n",
      "  [4.57732936e-01]\n",
      "  [4.58174057e-01]\n",
      "  [4.59209937e-01]\n",
      "  [4.59744666e-01]\n",
      "  [7.20935136e-03]\n",
      "  [7.21292886e-03]\n",
      "  [6.85966195e-03]\n",
      "  [6.75792558e-03]\n",
      "  [6.75745280e-03]\n",
      "  [6.76015651e-03]\n",
      "  [6.75637331e-03]\n",
      "  [6.75631024e-03]\n",
      "  [6.75276910e-03]\n",
      "  [6.75618969e-03]\n",
      "  [6.79235406e-03]\n",
      "  [6.80158282e-03]\n",
      "  [1.04073123e-01]\n",
      "  [2.10200566e-02]\n",
      "  [7.78877600e-01]\n",
      "  [2.27080486e-02]\n",
      "  [4.80815501e-02]\n",
      "  [3.75582983e-03]\n",
      "  [5.66923223e-03]\n",
      "  [1.23828376e-02]\n",
      "  [5.91122231e-03]\n",
      "  [2.03363097e-03]\n",
      "  [8.66113363e-02]\n",
      "  [8.42161588e-02]\n",
      "  [3.88273754e-03]\n",
      "  [7.84211627e-04]\n",
      "  [2.90581976e-02]\n",
      "  [8.47186871e-04]\n",
      "  [1.79381600e-03]\n",
      "  [1.40121575e-04]\n",
      "  [2.11506380e-04]\n",
      "  [4.61976122e-04]\n",
      "  [2.20534494e-04]\n",
      "  [7.58701514e-05]\n",
      "  [3.23127696e-03]\n",
      "  [3.14191820e-03]\n",
      "  [3.43594226e-03]\n",
      "  [6.93970637e-04]\n",
      "  [2.57144060e-02]\n",
      "  [7.49699179e-04]\n",
      "  [1.58739758e-03]\n",
      "  [1.23997464e-04]\n",
      "  [1.87167860e-04]\n",
      "  [4.08815490e-04]\n",
      "  [1.95157088e-04]\n",
      "  [6.71395946e-05]\n",
      "  [2.85944670e-03]\n",
      "  [2.78037065e-03]\n",
      "  [1.74071025e-09]\n",
      "  [2.09479545e-10]\n",
      "  [3.84309978e-08]\n",
      "  [3.02382670e-11]\n",
      "  [2.19606390e-09]\n",
      "  [8.10165571e-11]\n",
      "  [1.56554178e-09]\n",
      "  [2.26538621e-09]\n",
      "  [4.95111999e-10]\n",
      "  [3.56813445e-10]\n",
      "  [4.41721849e-08]\n",
      "  [1.00847937e-09]\n",
      "  [3.10000000e-01]\n",
      "  [3.00000000e-01]\n",
      "  [3.20000000e-01]\n",
      "  [3.40000000e-01]\n",
      "  [2.90000000e-01]\n",
      "  [2.90000000e-01]\n",
      "  [2.90000000e-01]\n",
      "  [2.90000000e-01]\n",
      "  [2.90000000e-01]\n",
      "  [2.70000000e-01]\n",
      "  [2.70000000e-01]\n",
      "  [2.80000000e-01]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [5.20000000e-01]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [6.40000000e-01]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [5.20000000e-01]\n",
      "  [0.00000000e+00]]\n",
      "\n",
      " [[0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [9.84285178e-01]\n",
      "  [9.81692232e-01]\n",
      "  [9.73481237e-01]\n",
      "  [9.73481237e-01]\n",
      "  [9.68609642e-01]\n",
      "  [9.73481237e-01]\n",
      "  [9.73481237e-01]\n",
      "  [9.73481237e-01]\n",
      "  [9.80199324e-01]\n",
      "  [9.99960713e-01]\n",
      "  [1.00000000e+00]\n",
      "  [9.93025511e-01]\n",
      "  [9.82659922e-01]\n",
      "  [9.81699817e-01]\n",
      "  [9.78659484e-01]\n",
      "  [9.78659484e-01]\n",
      "  [9.76855651e-01]\n",
      "  [9.78659484e-01]\n",
      "  [9.78659484e-01]\n",
      "  [9.78659484e-01]\n",
      "  [9.81147029e-01]\n",
      "  [9.88464193e-01]\n",
      "  [9.92260972e-01]\n",
      "  [9.89671598e-01]\n",
      "  [9.85589952e-01]\n",
      "  [9.84792080e-01]\n",
      "  [9.82265486e-01]\n",
      "  [9.82265486e-01]\n",
      "  [9.80766454e-01]\n",
      "  [9.82265486e-01]\n",
      "  [9.82265486e-01]\n",
      "  [9.82265486e-01]\n",
      "  [9.84332699e-01]\n",
      "  [9.90413450e-01]\n",
      "  [9.93568670e-01]\n",
      "  [9.91416834e-01]\n",
      "  [4.84087267e-01]\n",
      "  [4.86992347e-01]\n",
      "  [4.81714103e-01]\n",
      "  [4.89426040e-01]\n",
      "  [4.84840656e-01]\n",
      "  [4.94017306e-01]\n",
      "  [4.89426040e-01]\n",
      "  [4.89426040e-01]\n",
      "  [4.95749432e-01]\n",
      "  [5.07993621e-01]\n",
      "  [4.99010783e-01]\n",
      "  [4.82906778e-01]\n",
      "  [5.14385295e-01]\n",
      "  [5.20454406e-01]\n",
      "  [5.05894990e-01]\n",
      "  [5.19390992e-01]\n",
      "  [5.09862356e-01]\n",
      "  [4.96594373e-01]\n",
      "  [5.02362591e-01]\n",
      "  [5.03408968e-01]\n",
      "  [5.13449066e-01]\n",
      "  [5.22075471e-01]\n",
      "  [5.20171389e-01]\n",
      "  [5.11304899e-01]\n",
      "  [9.80755817e-02]\n",
      "  [9.59010468e-02]\n",
      "  [9.31030482e-02]\n",
      "  [8.54685927e-02]\n",
      "  [8.42805430e-02]\n",
      "  [7.54722944e-02]\n",
      "  [7.37959589e-02]\n",
      "  [7.36938278e-02]\n",
      "  [7.10045596e-02]\n",
      "  [7.32647949e-02]\n",
      "  [7.25908306e-02]\n",
      "  [7.22971289e-02]\n",
      "  [4.62577296e-01]\n",
      "  [4.62302598e-01]\n",
      "  [4.61650179e-01]\n",
      "  [4.62582360e-01]\n",
      "  [4.62076818e-01]\n",
      "  [4.62583009e-01]\n",
      "  [4.62295012e-01]\n",
      "  [4.62403732e-01]\n",
      "  [4.63100892e-01]\n",
      "  [4.65147985e-01]\n",
      "  [4.66204712e-01]\n",
      "  [4.65485957e-01]\n",
      "  [1.34502976e-05]\n",
      "  [7.00384301e-06]\n",
      "  [3.08011667e-05]\n",
      "  [0.00000000e+00]\n",
      "  [1.03094041e-05]\n",
      "  [4.87040750e-06]\n",
      "  [9.37247161e-07]\n",
      "  [0.00000000e+00]\n",
      "  [1.06485615e-05]\n",
      "  [1.13138101e-04]\n",
      "  [2.73111873e-05]\n",
      "  [1.92688685e-05]\n",
      "  [4.54386716e-01]\n",
      "  [4.57400917e-01]\n",
      "  [4.58707539e-01]\n",
      "  [4.58567370e-01]\n",
      "  [4.58311553e-01]\n",
      "  [4.58142818e-01]\n",
      "  [4.58072931e-01]\n",
      "  [4.57732936e-01]\n",
      "  [4.58174057e-01]\n",
      "  [4.59209937e-01]\n",
      "  [4.59744666e-01]\n",
      "  [4.59380959e-01]\n",
      "  [7.21292886e-03]\n",
      "  [6.85966195e-03]\n",
      "  [6.75792558e-03]\n",
      "  [6.75745280e-03]\n",
      "  [6.76015651e-03]\n",
      "  [6.75637331e-03]\n",
      "  [6.75631024e-03]\n",
      "  [6.75276910e-03]\n",
      "  [6.75618969e-03]\n",
      "  [6.79235406e-03]\n",
      "  [6.80158282e-03]\n",
      "  [6.80681158e-03]\n",
      "  [2.10200566e-02]\n",
      "  [7.78877600e-01]\n",
      "  [2.27080486e-02]\n",
      "  [4.80815501e-02]\n",
      "  [3.75582983e-03]\n",
      "  [5.66923223e-03]\n",
      "  [1.23828376e-02]\n",
      "  [5.91122231e-03]\n",
      "  [2.03363097e-03]\n",
      "  [8.66113363e-02]\n",
      "  [8.42161588e-02]\n",
      "  [3.47807479e-02]\n",
      "  [7.84211627e-04]\n",
      "  [2.90581976e-02]\n",
      "  [8.47186871e-04]\n",
      "  [1.79381600e-03]\n",
      "  [1.40121575e-04]\n",
      "  [2.11506380e-04]\n",
      "  [4.61976122e-04]\n",
      "  [2.20534494e-04]\n",
      "  [7.58701514e-05]\n",
      "  [3.23127696e-03]\n",
      "  [3.14191820e-03]\n",
      "  [1.29759253e-03]\n",
      "  [6.93970637e-04]\n",
      "  [2.57144060e-02]\n",
      "  [7.49699179e-04]\n",
      "  [1.58739758e-03]\n",
      "  [1.23997464e-04]\n",
      "  [1.87167860e-04]\n",
      "  [4.08815490e-04]\n",
      "  [1.95157088e-04]\n",
      "  [6.71395946e-05]\n",
      "  [2.85944670e-03]\n",
      "  [2.78037065e-03]\n",
      "  [1.14827566e-03]\n",
      "  [2.09479545e-10]\n",
      "  [3.84309978e-08]\n",
      "  [3.02382670e-11]\n",
      "  [2.19606390e-09]\n",
      "  [8.10165571e-11]\n",
      "  [1.56554178e-09]\n",
      "  [2.26538621e-09]\n",
      "  [4.95111999e-10]\n",
      "  [3.56813445e-10]\n",
      "  [4.41721849e-08]\n",
      "  [1.00847937e-09]\n",
      "  [4.28341168e-10]\n",
      "  [3.00000000e-01]\n",
      "  [3.20000000e-01]\n",
      "  [3.40000000e-01]\n",
      "  [2.90000000e-01]\n",
      "  [2.90000000e-01]\n",
      "  [2.90000000e-01]\n",
      "  [2.90000000e-01]\n",
      "  [2.90000000e-01]\n",
      "  [2.70000000e-01]\n",
      "  [2.70000000e-01]\n",
      "  [2.80000000e-01]\n",
      "  [2.90000000e-01]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [1.00000000e-02]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [5.20000000e-01]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [6.40000000e-01]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [5.20000000e-01]\n",
      "  [0.00000000e+00]\n",
      "  [4.60000000e-01]]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keras LSTM API requires features data as a vertical vector\n",
    "\n",
    "# reshape training and test data\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print (f\"X_train sample values:\\n{X_train[:2]} \\n\")\n",
    "#print (f\"X_test sample values:\\n{X_test[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM RNN model definition\n",
    "\n",
    "model = Sequential()\n",
    "dropout_fraction = 0.25\n",
    "\n",
    "# Layer 1\n",
    "model.add(LSTM(\n",
    "    units=window_size,\n",
    "    return_sequences=True,\n",
    "    input_shape=(X_train.shape[1], 1))\n",
    "    )\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Layer 2\n",
    "model.add(LSTM(units=window_size, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Layer 3\n",
    "model.add(LSTM(units=window_size))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 240, 12)           672       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 240, 12)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 240, 12)           1200      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 240, 12)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 12)                1200      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 3,085\n",
      "Trainable params: 3,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model compilation and summary\n",
    "\n",
    "# the output value is not continuous rather boolean so different loss parameter \n",
    "model.compile(loss=\"binary_crossentropy\", optimizer = \"adam\", metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 1072/24850 [>.............................] - ETA: 45:12 - loss: 3.8993e-06 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-91c5a7ab20b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Training time!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training time!\n",
    "model.fit(X_train, y_train, epochs=5, shuffle = False, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance with test data\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
