{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hvplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports to get show started\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from numpy.random import seed\n",
    "from pathlib import Path\n",
    "\n",
    "# Import required preprocessing and Keras modules\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO after model experimentation\n",
    "# Define random seed for reproducibility\n",
    "\n",
    "# seed(1)\n",
    "# random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 35513 entries, 2017-01-01 00:00:00 to 2021-01-15 00:00:00\n",
      "Data columns (total 24 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Close                 35513 non-null  float64\n",
      " 1   Volume                35513 non-null  float64\n",
      " 2   US_Holiday            35513 non-null  float64\n",
      " 3   US_Market_Open        35513 non-null  float64\n",
      " 4   Trail24hr_CloseRatio  35513 non-null  float64\n",
      " 5   Trail12Wk_CloseRatio  35513 non-null  float64\n",
      " 6   Trail52Wk_CloseRatio  35513 non-null  float64\n",
      " 7   Hr_Return             35513 non-null  float64\n",
      " 8   Trail24hr_Return      35513 non-null  float64\n",
      " 9   Trail24hr_Std         35513 non-null  float64\n",
      " 10  Trail12Wk_Return      35513 non-null  float64\n",
      " 11  Trail12Wk_Std         35513 non-null  float64\n",
      " 12  Trail52Wk_Return      35513 non-null  float64\n",
      " 13  Trail52Wk_Std         35513 non-null  float64\n",
      " 14  Trail24hr_VolRatio    35513 non-null  float64\n",
      " 15  Trail12Wk_VolRatio    35513 non-null  float64\n",
      " 16  Trail52Wk_VolRatio    35513 non-null  float64\n",
      " 17  Vol_PctDelta          35513 non-null  float64\n",
      " 18  Bitcoin Trend         35513 non-null  float64\n",
      " 19  Cryptocurrency        35513 non-null  float64\n",
      " 20  crypto all time high  35513 non-null  float64\n",
      " 21  crypto drop           35513 non-null  float64\n",
      " 22  btc halving           35513 non-null  float64\n",
      " 23  Significant_Drawdown  35513 non-null  int64  \n",
      "dtypes: float64(23), int64(1)\n",
      "memory usage: 6.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Read in prepared model dataset created in data_prep notebook \n",
    "\n",
    "model_df = pd.read_csv(Path('./ModelData/model_dataset.csv'),index_col=\"Date_Time\",infer_datetime_format=True,parse_dates=True)\n",
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 35513 entries, 2017-01-01 00:00:00 to 2021-01-15 00:00:00\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   US_Market_Open        35513 non-null  float64\n",
      " 1   Trail24hr_CloseRatio  35513 non-null  float64\n",
      " 2   Trail12Wk_CloseRatio  35513 non-null  float64\n",
      " 3   Trail52Wk_CloseRatio  35513 non-null  float64\n",
      " 4   Hr_Return             35513 non-null  float64\n",
      " 5   Trail24hr_Return      35513 non-null  float64\n",
      " 6   Trail24hr_Std         35513 non-null  float64\n",
      " 7   Trail12Wk_Return      35513 non-null  float64\n",
      " 8   Trail12Wk_Std         35513 non-null  float64\n",
      " 9   Trail52Wk_Return      35513 non-null  float64\n",
      " 10  Trail52Wk_Std         35513 non-null  float64\n",
      " 11  Trail24hr_VolRatio    35513 non-null  float64\n",
      " 12  Trail12Wk_VolRatio    35513 non-null  float64\n",
      " 13  Trail52Wk_VolRatio    35513 non-null  float64\n",
      " 14  Vol_PctDelta          35513 non-null  float64\n",
      " 15  Bitcoin Trend         35513 non-null  float64\n",
      " 16  Cryptocurrency        35513 non-null  float64\n",
      " 17  crypto all time high  35513 non-null  float64\n",
      " 18  crypto drop           35513 non-null  float64\n",
      " 19  btc halving           35513 non-null  float64\n",
      " 20  Significant_Drawdown  35513 non-null  int64  \n",
      "dtypes: float64(20), int64(1)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Last minute pruning of unwanted columns\n",
    "#  remove Close price, US holiday\n",
    "column_2drop_list = ['Close',\n",
    "                     'Volume',\n",
    "                     'US_Holiday']\n",
    "\n",
    "# old: remove 12week numbers to reduce model fittin time for initial model evals\n",
    "#column_2drop_list = ['Close',\n",
    "#                     'US_Holiday',\n",
    "#                     'Trail12Wk_CloseRatio',\n",
    "#                     'Trail12Wk_Return',\n",
    "#                     'Trail12Wk_Std',\n",
    "#                     'Trail12Wk_VolRatio',\n",
    "#                     'Vol_PctDelta']\n",
    "\n",
    "model_df = model_df.drop(columns=column_2drop_list)\n",
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_window_chopper(df, window_len, feature_col_numlist, target_col_num):\n",
    "    \"\"\"\n",
    "    Function chops up dataframe features (X) defined by column numbers\n",
    "    in feature_col_numlist and target (y) values defined by t_col_num\n",
    "    with a rolling window of length window_len.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df) - window_len):\n",
    "        feature_list = []\n",
    "        for feature_col_num in feature_col_numlist:\n",
    "            feature_list.append(df.iloc[i:(i + window_len), feature_col_num])\n",
    "        X.append(feature_list)\n",
    "        y.append(df.iloc[(i + window_len), target_col_num])\n",
    "    return np.array(X).reshape(-1,(len(feature_col_numlist)*window_len)), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X sample values:\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   9.97383408e-01  9.95895542e-01  9.95218306e-01  9.93073726e-01\n",
      "   9.93073726e-01  9.91801344e-01  9.93073726e-01  9.93073726e-01\n",
      "   9.93073726e-01  9.94828382e-01  9.99989739e-01  1.00000000e+00\n",
      "   9.98178376e-01  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  9.98475212e-01  1.00000000e+00  9.99509215e-01\n",
      "   1.00000000e+00  1.00000000e+00  9.98674692e-01  9.95047532e-01\n",
      "   9.89343186e-01  9.87867314e-01  9.87195538e-01  9.85068246e-01\n",
      "   9.85068246e-01  9.83806121e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.85068246e-01  9.86808758e-01  9.91928507e-01  9.94585076e-01\n",
      "   9.92773316e-01  9.96468086e-01  9.97435036e-01  1.00000000e+00\n",
      "   1.00000000e+00  9.98475212e-01  1.00000000e+00  9.99509215e-01\n",
      "   1.00000000e+00  1.00000000e+00  9.98674692e-01  9.95047532e-01\n",
      "   9.89343186e-01  9.87867314e-01  9.87195538e-01  9.85068246e-01\n",
      "   9.85068246e-01  9.83806121e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.85068246e-01  9.86808758e-01  9.91928507e-01  9.94585076e-01\n",
      "   9.92773316e-01  9.96468086e-01  9.97435036e-01  1.00000000e+00\n",
      "   1.00000000e+00  9.98475212e-01  1.00000000e+00  9.99509215e-01\n",
      "   1.00000000e+00  1.00000000e+00  9.98674692e-01  9.95047532e-01\n",
      "  -2.61659227e-03 -1.49176955e-03 -6.80026789e-04 -2.15488354e-03\n",
      "   0.00000000e+00 -1.28125646e-03  1.28290018e-03  0.00000000e+00\n",
      "   0.00000000e+00  1.76689399e-03  5.18818785e-03  2.67818663e-03\n",
      "  -1.82162411e-03  3.72166460e-03  9.70377937e-04  4.33695597e-03\n",
      "   1.28632392e-02 -1.52478783e-03  3.07432636e-03 -4.90785256e-04\n",
      "   4.09355553e-03  1.54192386e-03 -1.32530841e-03 -3.63197334e-03\n",
      "   1.68718876e-02  1.56730020e-02  1.94189114e-02  1.04327089e-02\n",
      "   1.87625628e-02  1.28814032e-02  4.69228456e-03  8.25248094e-03\n",
      "   8.89831428e-03  1.50951526e-02  2.04194476e-02  1.92442311e-02\n",
      "   1.37717537e-02  1.99253957e-02  1.96872389e-02  1.99034872e-02\n",
      "   3.27356370e-02  3.01745690e-02  2.71618759e-02  3.70428481e-02\n",
      "   3.25690449e-02  3.41522046e-02  3.08681131e-02  2.44992045e-02\n",
      "   4.92239243e-03  4.93899354e-03  4.82948602e-03  4.68858146e-03\n",
      "   4.30411750e-03  4.24428844e-03  3.80071337e-03  3.71629470e-03\n",
      "   3.71115147e-03  3.57572247e-03  3.68954579e-03  3.65560558e-03\n",
      "   3.64081504e-03  3.63661065e-03  3.63585832e-03  3.64465029e-03\n",
      "   4.38791415e-03  4.42719612e-03  4.32585032e-03  3.59124862e-03\n",
      "   3.31640415e-03  3.30310066e-03  3.34766439e-03  3.47757421e-03\n",
      "   4.67177792e-01  4.65216760e-01  4.64520559e-01  4.62867058e-01\n",
      "   4.65229594e-01  4.63948338e-01  4.65231238e-01  4.64501335e-01\n",
      "   4.64776875e-01  4.66543769e-01  4.71731957e-01  4.74410143e-01\n",
      "   4.72588519e-01  4.76310184e-01  4.76129459e-01  4.79802458e-01\n",
      "   4.93280664e-01  4.91739683e-01  4.94797816e-01  4.95505295e-01\n",
      "   4.99712336e-01  5.02454098e-01  5.01128789e-01  4.97529283e-01\n",
      "   3.76268101e-03  3.76287306e-03  3.76292471e-03  3.76326448e-03\n",
      "   3.76282471e-03  3.76297190e-03  3.76304144e-03  3.76302851e-03\n",
      "   3.76301513e-03  3.76316716e-03  3.76478249e-03  3.76517238e-03\n",
      "   3.76544746e-03  3.76624469e-03  3.76622505e-03  3.76732035e-03\n",
      "   3.77775091e-03  3.77795305e-03  3.77847549e-03  3.77837419e-03\n",
      "   3.77933733e-03  3.77930942e-03  3.77946806e-03  3.78045099e-03\n",
      "   9.51089704e-01  9.49597934e-01  9.64694523e-01  9.71238721e-01\n",
      "   9.70536688e-01  9.69255431e-01  9.68410323e-01  9.68060298e-01\n",
      "   9.66357435e-01  9.68566787e-01  9.73754975e-01  9.76433161e-01\n",
      "   9.74611537e-01  9.86207768e-01  9.87178146e-01  9.91515102e-01\n",
      "   1.04366447e+00  1.01065748e+00  1.01136214e+00  1.04370823e+00\n",
      "   1.04780179e+00  1.01231211e+00  1.01098680e+00  1.00735483e+00\n",
      "   5.90840562e-03  5.90843033e-03  5.90599089e-03  5.90528836e-03\n",
      "   5.90528510e-03  5.90530377e-03  5.90527765e-03  5.90527721e-03\n",
      "   5.90525276e-03  5.90527638e-03  5.90552611e-03  5.90558983e-03\n",
      "   5.90562594e-03  5.90513391e-03  5.90514092e-03  5.90531372e-03\n",
      "   5.89182402e-03  5.88228515e-03  5.88232089e-03  5.87174711e-03\n",
      "   5.87190092e-03  5.85862245e-03  5.85864261e-03  5.85877969e-03\n",
      "   1.04073123e-01  2.10200568e-02  7.78877600e-01  2.27080488e-02\n",
      "   4.80815503e-02  3.75583002e-03  5.66923242e-03  1.23828378e-02\n",
      "   5.91122250e-03  2.03363117e-03  8.66113364e-02  8.42161590e-02\n",
      "   3.47807481e-02  1.95939055e-02  7.99384591e-02  6.42598132e-01\n",
      "   1.00000000e+00  4.81279528e-01  2.32523270e-01  2.09733872e-01\n",
      "   8.07986789e-01  1.33330083e-01  1.28194226e-01  1.53961091e-01\n",
      "   3.88273766e-03  7.84211749e-04  2.90581977e-02  8.47186993e-04\n",
      "   1.79381612e-03  1.40121697e-04  2.11506502e-04  4.61976244e-04\n",
      "   2.20534616e-04  7.58702735e-05  3.23127708e-03  3.14191832e-03\n",
      "   1.29759266e-03  5.69363581e-04  2.32286755e-03  1.86727436e-02\n",
      "   9.97597514e-02  4.80123261e-02  2.31964636e-02  2.09229989e-02\n",
      "   8.06045612e-02  1.33009759e-02  1.27886242e-02  1.53591201e-02\n",
      "   3.43594238e-03  6.93970754e-04  2.57144061e-02  7.49699296e-04\n",
      "   1.58739770e-03  1.23997581e-04  1.87167977e-04  4.08815607e-04\n",
      "   1.95157206e-04  6.71397119e-05  2.85944682e-03  2.78037077e-03\n",
      "   1.14827577e-03  5.03845644e-04  2.05557000e-03  1.65240293e-02\n",
      "   8.82801743e-02  4.24874406e-02  2.05271948e-02  1.85153428e-02\n",
      "   7.13292145e-02  1.17704030e-02  1.13170086e-02  1.35917119e-02\n",
      "   6.78340815e-01 -7.98026079e-01  3.60540198e+01 -9.70845164e-01\n",
      "   1.11737920e+00 -9.21886254e-01  5.09448615e-01  1.18421770e+00\n",
      "  -5.22627801e-01 -6.55971135e-01  4.15895009e+01 -2.76543179e-02\n",
      "  -5.87006241e-01 -5.61215472e-01  3.07976138e+00  7.03866047e+00\n",
      "   4.34253315e+00 -5.18720472e-01 -5.16864408e-01 -9.80091061e-02\n",
      "   2.85243824e+00 -8.34984823e-01 -3.85198634e-02  2.00998633e-01\n",
      "   3.10000000e+01  3.00000000e+01  3.20000000e+01  3.40000000e+01\n",
      "   2.90000000e+01  2.90000000e+01  2.90000000e+01  2.90000000e+01\n",
      "   2.90000000e+01  2.70000000e+01  2.70000000e+01  2.80000000e+01\n",
      "   2.90000000e+01  2.80000000e+01  3.00000000e+01  3.10000000e+01\n",
      "   3.40000000e+01  4.10000000e+01  4.30000000e+01  4.30000000e+01\n",
      "   4.90000000e+01  5.50000000e+01  5.30000000e+01  5.40000000e+01\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  5.20000000e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  3.70000000e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  6.40000000e+01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  5.20000000e+01  0.00000000e+00\n",
      "   4.60000000e+01  4.20000000e+01  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  4.20000000e+01  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   9.95895542e-01  9.95218306e-01  9.93073726e-01  9.93073726e-01\n",
      "   9.91801344e-01  9.93073726e-01  9.93073726e-01  9.93073726e-01\n",
      "   9.94828382e-01  9.99989739e-01  1.00000000e+00  9.98178376e-01\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   9.98475212e-01  1.00000000e+00  9.99509215e-01  1.00000000e+00\n",
      "   1.00000000e+00  9.98674692e-01  9.95047532e-01  9.94180601e-01\n",
      "   9.87867314e-01  9.87195538e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.83806121e-01  9.85068246e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.86808758e-01  9.91928507e-01  9.94585076e-01  9.92773316e-01\n",
      "   9.96468086e-01  9.97435036e-01  1.00000000e+00  1.00000000e+00\n",
      "   9.98475212e-01  1.00000000e+00  9.99509215e-01  1.00000000e+00\n",
      "   1.00000000e+00  9.98674692e-01  9.95047532e-01  9.94180601e-01\n",
      "   9.87867314e-01  9.87195538e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.83806121e-01  9.85068246e-01  9.85068246e-01  9.85068246e-01\n",
      "   9.86808758e-01  9.91928507e-01  9.94585076e-01  9.92773316e-01\n",
      "   9.96468086e-01  9.97435036e-01  1.00000000e+00  1.00000000e+00\n",
      "   9.98475212e-01  1.00000000e+00  9.99509215e-01  1.00000000e+00\n",
      "   1.00000000e+00  9.98674692e-01  9.95047532e-01  9.94180601e-01\n",
      "  -1.49176955e-03 -6.80026789e-04 -2.15488354e-03  0.00000000e+00\n",
      "  -1.28125646e-03  1.28290018e-03  0.00000000e+00  0.00000000e+00\n",
      "   1.76689399e-03  5.18818785e-03  2.67818663e-03 -1.82162411e-03\n",
      "   3.72166460e-03  9.70377937e-04  4.33695597e-03  1.28632392e-02\n",
      "  -1.52478783e-03  3.07432636e-03 -4.90785256e-04  4.09355553e-03\n",
      "   1.54192386e-03 -1.32530841e-03 -3.63197334e-03 -8.71245882e-04\n",
      "   1.56730020e-02  1.94189114e-02  1.04327089e-02  1.87625628e-02\n",
      "   1.28814032e-02  4.69228456e-03  8.25248094e-03  8.89831428e-03\n",
      "   1.50951526e-02  2.04194476e-02  1.92442311e-02  1.37717537e-02\n",
      "   1.99253957e-02  1.96872389e-02  1.99034872e-02  3.27356370e-02\n",
      "   3.01745690e-02  2.71618759e-02  3.70428481e-02  3.25690449e-02\n",
      "   3.41522046e-02  3.08681131e-02  2.44992045e-02  2.62445509e-02\n",
      "   4.93899354e-03  4.82948602e-03  4.68858146e-03  4.30411750e-03\n",
      "   4.24428844e-03  3.80071337e-03  3.71629470e-03  3.71115147e-03\n",
      "   3.57572247e-03  3.68954579e-03  3.65560558e-03  3.64081504e-03\n",
      "   3.63661065e-03  3.63585832e-03  3.64465029e-03  4.38791415e-03\n",
      "   4.42719612e-03  4.32585032e-03  3.59124862e-03  3.31640415e-03\n",
      "   3.30310066e-03  3.34766439e-03  3.47757421e-03  3.41590463e-03\n",
      "   4.65216760e-01  4.64520559e-01  4.62867058e-01  4.65229594e-01\n",
      "   4.63948338e-01  4.65231238e-01  4.64501335e-01  4.64776875e-01\n",
      "   4.66543769e-01  4.71731957e-01  4.74410143e-01  4.72588519e-01\n",
      "   4.76310184e-01  4.76129459e-01  4.79802458e-01  4.93280664e-01\n",
      "   4.91739683e-01  4.94797816e-01  4.95505295e-01  4.99712336e-01\n",
      "   5.02454098e-01  5.01128789e-01  4.97529283e-01  4.96658037e-01\n",
      "   3.76287306e-03  3.76292471e-03  3.76326448e-03  3.76282471e-03\n",
      "   3.76297190e-03  3.76304144e-03  3.76302851e-03  3.76301513e-03\n",
      "   3.76316716e-03  3.76478249e-03  3.76517238e-03  3.76544746e-03\n",
      "   3.76624469e-03  3.76622505e-03  3.76732035e-03  3.77775091e-03\n",
      "   3.77795305e-03  3.77847549e-03  3.77837419e-03  3.77933733e-03\n",
      "   3.77930942e-03  3.77946806e-03  3.78045099e-03  3.78052902e-03\n",
      "   9.49597934e-01  9.64694523e-01  9.71238721e-01  9.70536688e-01\n",
      "   9.69255431e-01  9.68410323e-01  9.68060298e-01  9.66357435e-01\n",
      "   9.68566787e-01  9.73754975e-01  9.76433161e-01  9.74611537e-01\n",
      "   9.86207768e-01  9.87178146e-01  9.91515102e-01  1.04366447e+00\n",
      "   1.01065748e+00  1.01136214e+00  1.04370823e+00  1.04780179e+00\n",
      "   1.01231211e+00  1.01098680e+00  1.00735483e+00  1.00648358e+00\n",
      "   5.90843033e-03  5.90599089e-03  5.90528836e-03  5.90528510e-03\n",
      "   5.90530377e-03  5.90527765e-03  5.90527721e-03  5.90525276e-03\n",
      "   5.90527638e-03  5.90552611e-03  5.90558983e-03  5.90562594e-03\n",
      "   5.90513391e-03  5.90514092e-03  5.90531372e-03  5.89182402e-03\n",
      "   5.88228515e-03  5.88232089e-03  5.87174711e-03  5.87190092e-03\n",
      "   5.85862245e-03  5.85864261e-03  5.85877969e-03  5.85878907e-03\n",
      "   2.10200568e-02  7.78877600e-01  2.27080488e-02  4.80815503e-02\n",
      "   3.75583002e-03  5.66923242e-03  1.23828378e-02  5.91122250e-03\n",
      "   2.03363117e-03  8.66113364e-02  8.42161590e-02  3.47807481e-02\n",
      "   1.95939055e-02  7.99384591e-02  6.42598132e-01  1.00000000e+00\n",
      "   4.81279528e-01  2.32523270e-01  2.09733872e-01  8.07986789e-01\n",
      "   1.33330083e-01  1.28194226e-01  1.53961091e-01  1.49680819e-01\n",
      "   7.84211749e-04  2.90581977e-02  8.47186993e-04  1.79381612e-03\n",
      "   1.40121697e-04  2.11506502e-04  4.61976244e-04  2.20534616e-04\n",
      "   7.58702735e-05  3.23127708e-03  3.14191832e-03  1.29759266e-03\n",
      "   5.69363581e-04  2.32286755e-03  1.86727436e-02  9.97597514e-02\n",
      "   4.80123261e-02  2.31964636e-02  2.09229989e-02  8.06045612e-02\n",
      "   1.33009759e-02  1.27886242e-02  1.53591201e-02  1.49321213e-02\n",
      "   6.93970754e-04  2.57144061e-02  7.49699296e-04  1.58739770e-03\n",
      "   1.23997581e-04  1.87167977e-04  4.08815607e-04  1.95157206e-04\n",
      "   6.71397119e-05  2.85944682e-03  2.78037077e-03  1.14827577e-03\n",
      "   5.03845644e-04  2.05557000e-03  1.65240293e-02  8.82801743e-02\n",
      "   4.24874406e-02  2.05271948e-02  1.85153428e-02  7.13292145e-02\n",
      "   1.17704030e-02  1.13170086e-02  1.35917119e-02  1.32138488e-02\n",
      "  -7.98026079e-01  3.60540198e+01 -9.70845164e-01  1.11737920e+00\n",
      "  -9.21886254e-01  5.09448615e-01  1.18421770e+00 -5.22627801e-01\n",
      "  -6.55971135e-01  4.15895009e+01 -2.76543179e-02 -5.87006241e-01\n",
      "  -5.61215472e-01  3.07976138e+00  7.03866047e+00  4.34253315e+00\n",
      "  -5.18720472e-01 -5.16864408e-01 -9.80091061e-02  2.85243824e+00\n",
      "  -8.34984823e-01 -3.85198634e-02  2.00998633e-01 -2.78009960e-02\n",
      "   3.00000000e+01  3.20000000e+01  3.40000000e+01  2.90000000e+01\n",
      "   2.90000000e+01  2.90000000e+01  2.90000000e+01  2.90000000e+01\n",
      "   2.70000000e+01  2.70000000e+01  2.80000000e+01  2.90000000e+01\n",
      "   2.80000000e+01  3.00000000e+01  3.10000000e+01  3.40000000e+01\n",
      "   4.10000000e+01  4.30000000e+01  4.30000000e+01  4.90000000e+01\n",
      "   5.50000000e+01  5.30000000e+01  5.40000000e+01  4.90000000e+01\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  5.20000000e+01  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  3.70000000e+01  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  6.40000000e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  5.20000000e+01  0.00000000e+00  4.60000000e+01\n",
      "   4.20000000e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   4.20000000e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Create features (X) and target (y) data window sets\n",
    "\n",
    "window_size = 24 # model dataset is hourly, try full day\n",
    "feature_col_numlist = list(range(model_df.shape[1]-1))\n",
    "target_col_num = (model_df.shape[1] - 1) # 0s based column index\n",
    "X, y = data_window_chopper(model_df, window_size, feature_col_numlist, target_col_num)\n",
    "\n",
    "print(f\"X sample values:\\n {X[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 70% of the data for training, 30% for testing\n",
    "split = int(0.7 * len(X))\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the MinMaxScaler to scale data between 0 and 1.\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# target is already boolean, doesnt need scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras LSTM API requires features data as a vertical vector\n",
    "\n",
    "# reshape training and test data\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "#print (f\"X_train sample values:\\n{X_train[:2]} \\n\")\n",
    "#print (f\"X_test sample values:\\n{X_test[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM RNN model definition\n",
    "\n",
    "model = Sequential()\n",
    "dropout_fraction = 0.4\n",
    "\n",
    "# Layer 1\n",
    "model.add(LSTM(\n",
    "    units=window_size,\n",
    "    return_sequences=True,\n",
    "    activation=\"sigmoid\",\n",
    "    input_shape=(X_train.shape[1], 1))\n",
    "    )\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Layer 2\n",
    "model.add(LSTM(units=window_size, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Layer 3\n",
    "model.add(LSTM(units=window_size, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Output layer\n",
    "model.add(Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, 480, 24)           2496      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 480, 24)           0         \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 480, 24)           4704      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 480, 24)           0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 480, 24)           4704      \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 480, 24)           0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 480, 1)            25        \n",
      "=================================================================\n",
      "Total params: 11,929\n",
      "Trainable params: 11,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model compilation and summary\n",
    "\n",
    "# the output value is not continuous rather boolean so different loss parameter \n",
    "model.compile(loss=\"binary_crossentropy\", optimizer = \"adam\", metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12421/12421 [==============================] - 8610s 692ms/step - loss: 0.0171 - accuracy: 0.9986\n",
      "Epoch 2/10\n",
      "12421/12421 [==============================] - 8555s 689ms/step - loss: 0.0122 - accuracy: 0.9987\n",
      "Epoch 3/10\n",
      "12421/12421 [==============================] - 9266s 746ms/step - loss: 0.0118 - accuracy: 0.9987\n",
      "Epoch 4/10\n",
      "12421/12421 [==============================] - 9454s 761ms/step - loss: 0.0115 - accuracy: 0.9987\n",
      "Epoch 5/10\n",
      "12421/12421 [==============================] - 5821s 469ms/step - loss: 0.0113 - accuracy: 0.9987\n",
      "Epoch 6/10\n",
      " 5265/12421 [===========>..................] - ETA: 1:02:03 - loss: 0.0082 - accuracy: 0.9991"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-407e08a21dba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Training time!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training time!\n",
    "model.fit(X_train, y_train, epochs=10, shuffle = False, batch_size=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_30_layer_call_and_return_conditional_losses, lstm_cell_30_layer_call_fn, lstm_cell_31_layer_call_and_return_conditional_losses, lstm_cell_31_layer_call_fn, lstm_cell_30_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_30_layer_call_and_return_conditional_losses, lstm_cell_30_layer_call_fn, lstm_cell_31_layer_call_and_return_conditional_losses, lstm_cell_31_layer_call_fn, lstm_cell_30_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "# Save the model state for later finish or evaluation\n",
    "model.save(\"./Model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance with test data\n",
    "\n",
    "print(\"/n*** All done with training...Model2 saved! ***/n\")\n",
    "print(\"Please head over to eval_model2 notebook to reload model and run test data for eval and plot generation...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
